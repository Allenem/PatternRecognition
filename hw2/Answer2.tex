\documentclass[a4paper,11pt,onecolumn,oneside,UTF8]{article}

\usepackage{ctex}     % ä¸­æ–‡æ”¯æŒ
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{bm}       % å…¬å¼ä¸­çš„ç²—ä½“å­—ç¬¦ï¼ˆç”¨å‘½ä»¤\boldsymbolï¼‰
\usepackage{graphicx, subfig}
\usepackage{caption}
\usepackage{float}

\addtolength{\topmargin}{-54pt}
\setlength{\oddsidemargin}{-0.9cm}  % 3.17cm - 1 inch
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{17.00cm}
\setlength{\textheight}{24.00cm}    % 24.62

\begin{document}

\begin{center}
    \Large\textbf{Answer of Assignment 2}
\end{center}

\begin{flushright}
    2020E8017782032\_è’²å°§
\end{flushright}

\section*{è¯´æ˜}
\noindent ä½œä¸šç”¨ä¸­æ–‡æ’°å†™ï¼Œé¼“åŠ±ä½¿ç”¨LaTeXã€‚
\newline æ–‡æ¡£æŒ‰``å­¦å·\_å§“å.pdf''å‘½åæäº¤ã€‚
\newline æœ¬æ¬¡ä½œä¸šæˆªæ­¢æ—¶é—´ä¸º2020å¹´10æœˆ27æ—¥ï¼Œè¯·åˆ°è¯¾ç¨‹ç½‘ç«™åŠæ—¶æäº¤ã€‚

\section*{Question 1}
Let $x$ have a uniform density
$$
    p \left(x \mid \theta \right) \sim V\left(0,\theta\right) = \left\{\begin{array}{ll}
        1/\theta, & 0 \leq x \leq \theta; \\
        0,        & \text { otherwise }
    \end{array}
    \right.
$$
\begin{enumerate}
    \item  Suppose that $n$ samples $\textbf{D} = \left \{x_1 ,...,x_n \right \}$ are
          drawn independently according to $p \left(x \mid \theta \right)$. Show that
          the maximum likelihood estimate for $\theta$ is max[\textbf{D}], i.e., the value of
          the maximum element in \textbf{D}.
    \item  Suppose that $n = 5$ points are drawn from the distribution and the maximum value
          of which happens to be $\mathop{max}\limits_{k} x_k = 0.6$. Plot the likelihood
          $p \left(x \mid \theta \right)$ in the range $0 \leq \theta \leq 1$. Explain in words
          why you do not need to know the values of the other four points.
\end{enumerate}

\section*{Answer 1}
è§£ï¼š
\begin{enumerate}
    \item æˆ‘ä»¬ä½¿ç”¨æŒ‡æ ‡å‡½æ•°$I\left(\bullet\right)$ï¼Œå½“æ‹¬å·å†…çš„é€»è¾‘ä¸ºçœŸåˆ™å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚
          åˆ™å¯ä»¥å¾—åˆ°ä¼¼ç„¶å‡½æ•°:
          $$
              \begin{aligned}
                  p\left(\bm D \mid \theta \right) & = \prod\limits_{k=1}^np\left(x_k \mid \theta \right) \\
                                                   & = \prod\limits_{k=1}^n\frac{1}{\theta}I\left(0 \leq
                  x_k \leq \theta\right)                                                                  \\
                                                   & = \frac{1}{\theta^n}I\left(\theta \geq \mathop{max}
                  \limits_{k} x_k\right)I\left(\mathop{min}\limits_{k} x_k \geq 0\right)                  \\
              \end{aligned}
          $$
          æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¼¼ç„¶å‡½æ•°éš$\theta$å¢å¤§è€Œå‡å°ï¼Œè€Œ$\theta$æœ€å°å€¼ä¸º$\mathop{max}\limits_{k} x_k$\\
          $\therefore$ å¯¹äº$\theta$çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯ \textbf{D} ä¸­çš„æœ€å¤§å€¼ç‚¹ $max[\textbf{D}]ã€‚$

    \item
          $$
              \because \left\{\begin{array}{ll}
                  n = 5;                \\
                  0 \leq \theta \leq 1; \\
                  \mathop{max}\limits_{k} x_k = 0.6
              \end{array}
              \right.
              \therefore \left\{\begin{array}{ll}
                  p\left(\bm D \mid \theta \right) = 0,                  & 0\leq\theta< 0.6;    \\
                  p\left(\bm D \mid \theta \right) = \frac{1}{\theta^5}, & 0.6\leq\theta\leq 1; \\
              \end{array}
              \right.
          $$
          å›¾åƒå¦‚ä¸‹æ‰€ç¤º\\
          \begin{figure}[H]
              \centering
              \includegraphics[width=.8\textwidth]{hw2_1.png}            % hw2_1.pngæ˜¯å›¾ç‰‡æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
              \caption{ $p\left(\bm D \mid \theta \right)-\theta$å›¾ }    % captionæ˜¯å›¾ç‰‡çš„æ ‡é¢˜
              \label{img1}                                               % labelæ ‡å¿—ï¼Œæ–¹ä¾¿ä¸Šä¸‹æ–‡çš„å¼•ç”¨
          \end{figure}
          $\because p\left(\bm D \mid \theta \right)$ çš„å€¼åªä¸ $n ,\theta ,\mathop{max}\limits_{k} x_k = 0.6$ æœ‰å…³\\
          $\therefore$ æˆ‘ä»¬ä¸éœ€è¦çŸ¥é“å…¶ä»–å…ƒç´ å€¼ã€‚\\

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Queestion 2}
Assume we have training data from a Gaussian distribution of known covariance
$\bm\Sigma$ but unknown mean $\bm\mu$. Suppose further that this mean itself is random, and
characterized by a Gaussian density having mean $\bm m_0$ and covariance $\bm\Sigma_0$ .

\begin{enumerate}
    \item What is the MAP estimator for $\bm\mu$?
    \item Suppose we transform our coordinates by a linear transform $\bm x' = \bm{Ax}$, for nonsingular
          matrix \textbf{A}, and accordingly for other terms. Determine whether your MAP estimator
          gives the appropriate estimate for the transformed mean $\bm\mu'$. Explain.
\end{enumerate}

\section*{Answer 2}
è§£ï¼š
\begin{enumerate}
    \item ç”±é¢˜å¯çŸ¥ï¼š
          $$
              p\left(\bm\mu\right) = \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma_0|^\frac{1}{2}}
              exp[-\frac{1}{2}\left(\bm\mu-\bm m_0\right)^t\bm\Sigma_0^{-1}\left(\bm\mu-\bm m_0\right)]
          $$
          $$
              \begin{aligned}
                  \ln\left[p\left(\bm D \mid \bm\mu\right)\right]
                   & = \ln \prod\limits_{k=1}^n p\left(\bm x_k \mid \bm \mu \right)                              \\
                   & = \sum\limits_{k=1}^n \ln p\left(\bm x_k \mid \bm \mu \right)                               \\
                   & = \sum\limits_{k=1}^n \ln \{ \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma|^\frac{1}{2}}
                  exp[-\frac{1}{2}\left(\bm x_k-\bm\mu\right)^t\bm\Sigma^{-1}\left(\bm x_k-\bm \mu\right)] \}    \\
                   & = -\frac{n}{2}\ln\left[\left(2\pi\right)^d|\Sigma|\right]-\sum\limits_{k=1}^n \left[
                      \frac{1}{2}\left(\bm x_k-\bm\mu\right)^t\bm\Sigma^{-1}\left(\bm x_k-\bm \mu\right)\right]
              \end{aligned}
          $$
          $\therefore$å‡å€¼$\bm\mu$çš„MAPä¼°è®¡æ˜¯
          $$
              \begin{aligned}
                  \hat{\bm\mu} & = \mathop{arg} \mathop{max}\limits_{\bm\mu} \{\ln\left[p\left(\bm D
                  \mid \bm\mu\right)\right]\times p\left(\bm\mu\right)\}                                            \\
                               & = \mathop{arg} \mathop{max}\limits_{\bm\mu} \left\{\left[-\frac{n}{2}\ln
                  \left[\left(2\pi\right)^d|\bm\Sigma|\right]-\sum\limits_{k=1}^n \left[\frac{1}{2}\right.
                  \left(\bm x_k-\bm\mu\right)^t\bm\Sigma^{-1}\left(\bm x_k-\bm \mu\right)\right]\right.             \\
                               & \left.\times\left[\frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma_0|^\frac{1}{2}}
                      exp[-\frac{1}{2}\left(\bm\mu-\bm m_0\right)^t\bm\Sigma_0^{-1}\left(\bm\mu-\bm m_0\right)]
                      \right]\right\}
              \end{aligned}
          $$
    \item ç»è¿‡çº¿æ€§å˜æ¢$\bm x' = \bm{Ax}$ä¹‹åï¼Œè®­ç»ƒæ ·æœ¬çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µå˜ä¸º$\bm\mu, \bm\Sigma$ï¼š
          $$
              \bm\mu' = \mathcal{E} \left[\bm x'\right] = \mathcal{E} \left[\bm {Ax}\right]
              = \bm A\mathcal{E} \left[\bm x\right] = \bm{A\mu}
          $$
          $$
              \begin{aligned}
                  \bm\Sigma' & = \mathcal{E} \left[\left(\bm x'-\bm \mu'\right)\left(\bm x'-\bm\mu'
                  \right)^t\right]                                                                     \\
                             & = \mathcal{E} \left[\left(\bm {Ax} -\bm {A\mu} \right)
                  \left(\bm {Ax} -\bm{A\mu} \right)^t\right]                                           \\
                             & = \mathcal{E} \left[\bm A\left(\bm x -\bm\mu \right)\left(\bm x -\bm\mu
                  \right)^t\bm A^t\right]                                                              \\
                             & = \bm {A\Sigma A^t}
              \end{aligned}
          $$
          è½¬æ¢åçš„å‡å€¼ä¾ç„¶ä¸ºé«˜æ–¯åˆ†å¸ƒï¼š
          $$
              \begin{aligned}
                  p\left(\bm\mu'\right) & = \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma'_0|^\frac{1}{2}}
                  exp[-\frac{1}{2}\left(\bm\mu'-\bm m'_0\right)^t\bm\Sigma_0^{'-1}\left(\bm\mu'-\bm m'_0\right)] \\
                                        & = \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma'_0|^\frac{1}{2}}
                  exp[-\frac{1}{2}\left(\bm\mu-\bm m_0\right)^t\bm A^t\left(\bm A\bm\Sigma_0\bm A^t\right)^{-1}
                  \bm A\left(\bm\mu-\bm m_0\right)]                                                              \\
                                        & = \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma'_0|^\frac{1}{2}}
                  exp[-\frac{1}{2}\left(\bm\mu-\bm m_0\right)^t\bm A^t\bm \left(\bm A^t\right)^{-1}
                  \left(\bm\Sigma_0\right)^{-1}\left(\bm A\right)^{-1}\bm A\left(\bm\mu-\bm m_0\right)]          \\
                                        & = \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm A\bm\Sigma_0
                      \bm A^t|^\frac{1}{2}}exp[-\frac{1}{2}\left(\bm\mu-\bm m_0\right)^t\bm
                  \left(\bm\Sigma_0\right)^{-1}\left(\bm\mu-\bm m_0\right)]                                      \\
              \end{aligned}
          $$
          log-likelihood:
          $$
              \begin{aligned}
                  \begin{aligned}
                      \ln\left[p\left(\bm D' \mid \bm\mu'\right)\right]
                       & = \ln \prod\limits_{k=1}^np\left(\bm x'_k \mid \bm \mu' \right)                    \\
                       & = \sum\limits_{k=1}^n \ln p\left(\bm x'_k \mid \bm \mu' \right)                    \\
                       & = \sum\limits_{k=1}^n \ln \{ \frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm\Sigma'|^
                          \frac{1}{2}}exp[-\frac{1}{2}\left(\bm x'_k-\bm\mu'\right)^t\bm\Sigma^{'-1}
                      \left(\bm x'_k-\bm \mu'\right)] \}                                                    \\
                       & = -\frac{n}{2}\ln \left[\left(2\pi\right)^d|\bm\Sigma'|\right]-\sum\limits_{k=1}^n
                      \left[\frac{1}{2}\left(\bm x_k-\bm\mu\right)^t\bm A^t\left(\bm A\bm\Sigma\bm A^t
                      \right)^{-1}\bm A\left(\bm x_k-\bm \mu\right)\right]                                  \\
                       & = -\frac{n}{2}\ln \left[\left(2\pi\right)^d|\bm A\bm\Sigma\bm A^t|\right]
                      -\sum\limits_{k=1}^n \left[\frac{1}{2}\left(\bm x_k-\bm\mu\right)^t\bm A^t\bm
                          \left(\bm A^t\right)^{-1}\left(\bm\Sigma\right)^{-1}\left(\bm A\right)^{-1}
                      \bm A\left(\bm x_k-\bm \mu\right)\right]                                              \\
                       & = -\frac{n}{2}\ln \left[\left(2\pi\right)^d|\bm A\bm\Sigma\bm A^t|\right]
                      -\sum\limits_{k=1}^n \left[\frac{1}{2}\left(\bm x_k-\bm\mu\right)^t\bm
                          \Sigma^{-1}\left(\bm x_k-\bm \mu\right)\right]
                  \end{aligned}
              \end{aligned}
          $$
          æœ€æ–°çš„å¯¹$\bm\mu'$çš„MAPä¼°è®¡å¦‚ä¸‹ï¼š
          $$
              \begin{aligned}
                  \hat{\bm\mu'}
                   & = \mathop{arg} \mathop{max}\limits_{\bm\mu} \{\ln\left[p\left(\bm D \mid \bm\mu\right)\right]
                  \times p\left(\bm\mu\right)\}                                                                    \\
                   & = \mathop{arg} \mathop{max}\limits_{\bm\mu} \left\{\left[-\frac{n}{2}\ln \left[\left(2\pi
                          \right)^d|\bm A\bm\Sigma\bm A^t|\right]-\sum\limits_{k=1}^n \left[\frac{1}{2}
                  \left(\bm x_k-\bm\mu\right)^t\bm\Sigma^{-1}\left(\bm x_k-\bm \mu\right)\right]\right]\right.     \\
                   & \left.\times\left[\frac{1}{\left(2\pi\right)^\frac{d}{2}|\bm A\bm\Sigma_0\bm A^t|^
                      \frac{1}{2}}exp[-\frac{1}{2}\left(\bm\mu-\bm m_0\right)^t\bm\Sigma_0^{-1}\left(\bm\mu-
                  \bm m_0\right)]\right]\right\}
              \end{aligned}
          $$
          é€šè¿‡æ¯”è¾ƒ$\hat{\bm\mu}$ä¸$\hat{\bm\mu'}$ï¼Œå¯è§ä¸¤è€…è¡¨è¾¾å¼ç›¸åŒï¼Œæ•…å¯ç”¨MAPé€‚å½“ä¼°è®¡$\hat{\bm\mu'}$ã€‚
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Queestion 3}
Consider data $\bm D =  \left \{ \begin{pmatrix} 1\\1 \end{pmatrix},\begin{pmatrix} 3\\3 \end{pmatrix},
    \begin{pmatrix} 2\\\star \end{pmatrix} \right \} $, sampled from a two-dimensional (separable) distribution
$p(x_1,x_2) = p(x_1)p(x_2)$, with (1). As usual, $\ast$ represents a missing feature value.
$$
    p \left(x_1\right) \sim
    \begin{cases}
        \frac{1}{\theta_1}e^{-x_1/\theta_1}, & \mbox{if }x_1\ge 0 \\
        0,                                   & \mbox{otherwise}
    \end{cases}
    \quad and\quad
    p \left(x_2\right) \sim
    \begin{cases}
        \frac{1}{\theta_1}, & \mbox{if }0\leq x_2\leq\theta \\
        0,                  & \mbox{otherwise}
    \end{cases}
    \quad \left(1\right)
$$
\begin{enumerate}
    \item Start with an initial estimate $\theta^0 = \begin{pmatrix} 2\\4 \end{pmatrix}$
          and analytically calculate $ğ‘„(\theta,\theta^0)$â€” the E step
          in the EM algorithm. Be sure to consider the normalization of your distribution.
    \item Find the $\theta$ that maximizes your $ğ‘„(\theta,\theta^0)$ â€” the M step.
\end{enumerate}

\section*{Answer 3}
è§£ï¼š
$$
    \begin{aligned}
        Q\left(\theta;\theta^0\right)
         & = \mathcal{E}_{x_{32}}\left[\ln p\left(x_g,x_b;\theta\right)\mid\theta^0,D_g\right]               \\
         & = \int_{-\infty}^{+\infty}\left[\ln p\left(x_1\mid\theta\right)+\ln p\left(x_2\mid
        \theta\right)+\ln p\left(x_3\mid\theta\right)\right]p\left(x_{32}\mid\theta^0,x_{31}=2\right)dx_{32} \\
         & = \ln p\left(x_1\mid\theta\right)+\ln p\left(x_2\mid\theta\right)+\int_{-\infty}^{+\infty}
        p\left(x_3\mid\theta\right)p\left(x_{32}\mid\theta^0,x_{31}=2\right)dx_{32}                          \\
         & = \ln p\left(x_1\mid\theta\right)+\ln p\left(x_2\mid\theta\right)+\int_{-\infty}^{+\infty}
        p\left(x_3\mid\theta\right)\frac{p\left(\begin{pmatrix} 2\\x_{32} \end{pmatrix}\mid\theta^0\right)}
        {\underbrace{\int_{-\infty}^{+\infty}p\left(\begin{pmatrix} 2\\x'_{32} \end{pmatrix}
        \mid\theta^0\right)dx'_{32}}_{=1/\left(2e\right)}}dx_{32}                                            \\
         & = \ln p\left(x_1\mid\theta\right)+\ln p\left(x_2\mid\theta\right)+2e\int_{-\infty}^{+\infty}
        \ln\left(\frac{1}{\theta_1}e^{-\frac{2}{\theta_1}}\cdot\frac{1}{\theta_2}\right)\cdot
        \left(\frac{1}{2e}\frac{1}{4}\right)dx_{32}                                                          \\
         & = \ln\left(\frac{1}{\theta_1}e^{-\frac{1}{\theta_1}}\cdot\frac{1}{\theta_2}\right)+
        \ln\left(\frac{1}{\theta_1}e^{-\frac{3}{\theta_1}}\cdot\frac{1}{\theta_2}\right)+
        \frac{1}{4}\ln\left(\frac{1}{\theta_1}e^{-\frac{2}{\theta_1}}\cdot\frac{1}{\theta_2}\right)
        \int_{-\infty}^{+\infty}1dx_{32}                                                                     \\
         & = -2\ln\theta_1\theta_2-\frac{4}{\theta_1}\underbrace{-\frac{1}{4}\left(\ln\theta_1
        \theta_2+\frac{2}{\theta_2}\right)\int_{-\infty}^{+\infty}1dx_{32}}_{\equiv K}
    \end{aligned}
$$
Case 1: $3\leq\theta_2\leq4$,
$$
    \begin{aligned}
        Q\left(\theta;\theta^0\right)
         & = -2\ln\theta_1\theta_2-\frac{4}{\theta_1}-\frac{1}{4}\left(\ln\theta_1\theta_2+\frac{2}
        {\theta_2}\right)\int_{0}^{\theta_2}1dx_{32}                                                       \\
         & = -2\ln\theta_1\theta_2-\frac{4}{\theta_1}-\frac{\theta_2}{4}\left(\ln\theta_1\theta_2+\frac{2}
        {\theta_2}\right)
    \end{aligned}
$$
$$
    \begin{aligned}
        \frac{\partial Q}{\partial \theta_1}
         & = -2/\theta_1+4/\left(\theta_1^2\right)-\theta_2/\left(4\theta_1\right)+
        \theta_2/\left(2\theta_1^2\right)                                                            \\
         & = \frac{\left(-8\theta_1+16\right)+\left(-\theta_1\theta_2+2\theta_2\right)}{4\theta_1^2} \\
         & = \frac{8\left(2-\theta_1\right)+\theta_2\left(2-\theta_1\right)}{4\theta_1^2}            \\
         & = \frac{\left(8+\theta_2\right)\left(2-\theta_1\right)}{4\theta_1^2}
    \end{aligned}
$$
ä»¤$\frac{\partial Q}{\partial \theta_1} = 0$ï¼Œåˆ™$\theta_1=2$æˆ–$\theta_2=-8$ï¼Œè€Œ
$
    p \left(x_2\right) \sim
    \begin{cases}
        \frac{1}{\theta_1}, & \mbox{if }0\leq x_2\leq\theta \\
        0,                  & \mbox{otherwise}
    \end{cases}
$
$\therefore \theta_1=2 \\
    Q = -2\ln 2\theta_2-2-\frac{\theta_2}{4}\left(\ln 2\theta_2+1\right)
    = -\ln 2\theta_2\left(2+\frac{\theta_2}{4}\right)-\left(2+\frac{\theta_2}{4}\right)
    = -\left(2+\frac{\theta_2}{4}\right)\left(1+\ln 2\theta_2\right)
$ \\
å¯è§ï¼Œ$\theta_2$è¶Šå¤§ï¼ŒQè¶Šå°ï¼Œåˆ $3\leq\theta_2\leq4$ï¼Œ$\therefore$ å½“$\theta_2=3$ï¼Œæœ‰$Q_{max} \approx -7.677$ã€‚\\
Case 2: $\theta_2\geq 4$,
$$
    \begin{aligned}
        Q\left(\theta;\theta^0\right)
         & = -2\ln\theta_1\theta_2-\frac{4}{\theta_1}-\frac{1}{4}\left(\ln\theta_1\theta_2+\frac{2}
        {\theta_2}\right)\int_{0}^{4}1dx_{32}                                                       \\
         & = -2\ln\theta_1\theta_2-\frac{4}{\theta_1}-\left(\ln\theta_1\theta_2+\frac{2}
        {\theta_2}\right)                                                                           \\
         & = -3\ln\theta_1\theta_2-\frac{6}{\theta_1}
    \end{aligned}
$$
$$
    \begin{aligned}
        \frac{\partial Q}{\partial \theta_1}
         & = -3/\theta_1+6/\left(\theta_1^2\right) \\
         & = \frac{6-3\theta_1}{\theta_1^2}
    \end{aligned}
$$
ä»¤$\frac{\partial Q}{\partial \theta_1} = 0$ï¼Œåˆ™$\theta_1=2$ï¼Œ\\
$Q = -3\ln 2\theta_2-3$\\
å¯è§ï¼Œ$\theta_2$è¶Šå¤§ï¼ŒQè¶Šå°ï¼Œå½“$\theta_2=4$ï¼Œæœ‰$Q_{max} \approx -9.238$ã€‚\\
Case 3: $\theta_2=otherwise$, K=0ã€‚\\
ç»¼ä¸Šï¼Œ
$$
    Q_{max} = \begin{cases}
        -7.677, & \mbox{if }3\leq \theta_2\leq 4, \bm \theta = \left[ \begin{matrix}2\\3\end{matrix}\right] \\
        -9.238, & \mbox{if } \theta_2\geq 4, \bm \theta = \left[ \begin{matrix}2\\4\end{matrix}\right]
    \end{cases}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Queestion 4}
Consider training an HMM by the Forward-backward algorithm, for a single
squence of length T where each symbol could be one of c values. What is the
computational complexity of a single revision of all values $\hat{a}_{ij}$
and $\hat{b}_{jk}$?

\section*{Answer 4}

è§£ï¼š\\
æˆ‘ä»¬å®šä¹‰ä»çŠ¶æ€$\omega_i\left(t-1\right)$è½¬ç§»åˆ°$\omega_j\left(t\right)$çš„æ¦‚ç‡$\gamma_{ij}\left(t\right)$ï¼Œ\\
$$
    \gamma_{ij}\left(t\right) = \frac{\alpha_i\left(t-1\right)a_{ij}b_{jk}\beta_j\left(t\right)}{P
    \left(\bm V^T\mid\bm\theta\right)}
$$
å…¶ä¸­ï¼Œ$P\left(\bm V^T\mid\bm\theta\right)$æ˜¯æ¨¡å‹ç”¨ä»»æ„çš„éšçŠ¶æ€è·¯å¾„äº§ç”Ÿåºåˆ—$\bm V^T$çš„æ¦‚ç‡ã€‚
è¿™æ ·$\gamma_{ij}\left(t\right)$å°±æ˜¯åœ¨äº§ç”Ÿåºåˆ—çš„æ¡ä»¶ä¸‹ä»çŠ¶æ€$\omega_i\left(t-1\right)$è½¬ç§»åˆ°
$\omega_j\left(t\right)$çš„æ¦‚ç‡ã€‚\\
åºåˆ—ä»çŠ¶æ€$\omega_i\left(t-1\right)$è½¬ç§»åˆ°$\omega_j\left(t\right)$çš„é¢„æœŸå€¼æ˜¯
$\sum_{t=1}^T\gamma_{ij}\left(t\right)$ï¼Œè€Œä»$\omega_i$çš„ä»»ä½•è½¬ç§»çš„æ€»é¢„æœŸæ•°ä¸º
$\sum_{t=1}^T\sum_k\gamma_{ik}\left(t\right)$ï¼Œè¿™æ ·ï¼ŒéšçŠ¶æ€è½¬ç§»æ¦‚ç‡$\hat a_{ij}$å’Œ
å‘å‡ºæ˜¾çŠ¶æ€æ¦‚ç‡$\hat b_{jk}$å¯å¦‚æ­¤æ±‚å‡ºï¼š\\
$$
    \begin{aligned}
        \hat a_{ij} = \frac{\sum_{t=1}^T\gamma_{ij}\left(t\right)}
        {\sum_{t=1}^T\sum_k\gamma_{ik}\left(t\right)} \\
        \hat b_{jk} = \frac{\mathop{\sum_{t=1}^T}\limits_{v\left(t\right)=v_k}\sum_l\gamma_{jl}\left(t\right)}
        {\sum_{t=1}^T\sum_l\gamma_{jl}\left(t\right)}
    \end{aligned}
$$
å¯è§ï¼Œ$\hat{a}_{ij}$å’Œ$\hat{b}_{jk}$å¤æ‚åº¦éƒ½ä¸º$O\left(c^2T\right)$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Queestion 5}
Consider a normal $p\left(x\right)\sim N\left(\mu,\sigma^2\right)$ and Parzen-window function
$\phi\left(x\right)\sim N\left(0, 1\right)$ . Show that the Parzen-window estimate
$$
    p_n\left(x\right) = \frac{1}{nh_n}\sum\limits_{i=1}^n\phi\left(\frac{x-x_i}{h_n}\right)
$$
has the following properties: \\
1. $\bar p_n\left(x\right)\sim N\left(\mu,\sigma^2+h_n^2\right)$\\
2. $Var\left[p_n\left(x\right)\right] \approx \frac{1}{2nh_n\sqrt\pi}p\left(x\right)$

\section*{Answer 5}
è§£ï¼šç”±é¢˜å¯çŸ¥
$$
    \begin{aligned}
         & p\left(x\right) = \frac{1}{\sqrt{2\pi}\sigma}exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right] \\
         & \phi\left(x\right) = \frac{1}{\sqrt{2\pi}}exp\left[-\frac{x^2}{2}\right]                                     \\
         & p_n\left(x\right) = \frac{1}{nh_n}\sum\limits_{i=1}^n\phi\left(\frac{x-x_i}{h_n}\right)
    \end{aligned}
$$
\begin{enumerate}
    \item
          $$
              \begin{aligned}
                  \bar p_n\left(x\right) & = \mathcal{E}\left[p_n\left(x\right)\right]                      \\
                                         & = \frac{1}{nh_n}\sum\limits_{i=1}^n\mathcal{E}
                  \left[\phi\left(\frac{x-x_i}{h_n}\right)\right]                                           \\
                                         & = \frac{1}{h_n}\int_{-\infty}^{+\infty}\phi\left(
                  \frac{x-v}{h_n}\right)p\left(v\right)dv                                                   \\
                                         & = \frac{1}{h_n}\int_{-\infty}^{+\infty} \frac{1}{
                      \sqrt{2\pi}}exp\left[-\frac{1}{2}\left(\frac{x-v}{h_n}\right)^2\right]\frac{1}
                  {\sqrt{2\pi}\sigma}exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]dv--(1) \\
                                         & = \frac{1}{2\pi h_n\sigma}\int_{-\infty}^{+\infty}exp
                  \left[-\frac{1}{2}\left(\frac{x^2}{h_n^2}+\frac{\mu^2}{\sigma^2}\right)
                      -\frac{1}{2}v^2\underbrace{\left(\frac{1}{h_n^2}+\frac{1}{\sigma^2}\right)}
                      _{\equiv 1/\theta^2}+v\underbrace{\left(\frac{x}{h_n^2}+\frac{\mu}{\sigma}
                          \right)}_{\equiv \alpha/\theta^2}\right]dv
              \end{aligned}
          $$
          å³ä»¤
          $$
              \begin{cases}
                  \theta^2 = \frac{1}{1/h_n^2+1/\sigma^2} = \frac{h_n^2\sigma^2}{h_n^2+\sigma^2} \\
                  \alpha = \theta^2\left(\frac{x}{h_n^2}+\frac{\mu}{\sigma^2}\right)
              \end{cases}
          $$
          $$
              \begin{aligned}
                  \bar p_n\left(x\right) & = \frac{1}{2\pi h_n\sigma}exp\left[-\frac{1}{2}
                      \left(\frac{x^2}{h_n^2}+\frac{\mu^2}{\sigma^2}\right)\right]\int_{-\infty}^{+\infty}
                  exp\left[-\frac{1}{2}\left(\frac{v^2}{\theta^2}-\frac{2v\alpha}{\theta^2}
                  +\frac{\alpha^2}{\theta^2}-\frac{\alpha^2}{\theta^2}\right)\right]dv                       \\
                                         & = \frac{1}{2\pi h_n\sigma}exp\left[-\frac{1}{2}
                      \left(\frac{x^2}{h_n^2}+\frac{\mu^2}{\sigma^2}-\frac{\alpha^2}{\theta^2}
                      \right)\right]\int_{-\infty}^{+\infty}exp\left[-\frac{1}{2}
                  \left(\frac{v-\alpha}{\theta}\right)^2\right]dv                                            \\
                                         & = \frac{1}{2\pi h_n\sigma}exp\left[-\frac{1}{2}
                      \left(\frac{x^2}{h_n^2}+\frac{\mu^2}{\sigma^2}-\frac{\alpha^2}{\theta^2}
                      \right)\right]\int_{-\infty}^{+\infty}exp\left[-\left(\frac{w}{\sqrt{2}\theta}\right)^2
                  \right]dw                                                                                  \\
                                         & = \frac{1}{2\pi h_n\sigma}exp\left[-\frac{1}{2}
                      \left(\frac{x^2}{h_n^2}+\frac{\mu^2}{\sigma^2}-\frac{\alpha^2}{\theta^2}
                  \right)\right]\sqrt{\pi}\sqrt{2}\theta                                                     \\
                                         & = \frac{\theta}{\sqrt{2\pi} h_n\sigma}exp\left[-\frac{1}{2}
                      \left(\frac{x^2}{h_n^2}+\frac{\mu^2}{\sigma^2}-\frac{h_n^2\sigma^2}{h_n^2+\sigma^2}
                  \left(\frac{x}{h_n^2}+\frac{\mu}{\sigma^2}\right)^2\right)\right]                          \\
                                         & = \frac{1}{\sqrt{2\pi}\sqrt{h_n^2+\sigma^2}}exp\left[-\frac{1}{2}
                      \frac{\left(x-\mu\right)^2}{h_n^2+\sigma^2}\right]--(2)
              \end{aligned}
          $$
          ç”±æ­¤å¯è§ï¼Œ$\bar p_n\left(x\right)\sim N\left(\mu,\sigma^2+h_n^2\right)$ ã€‚
    \item
          $$
              \begin{aligned}
                  Var\left[p_n\left(x\right)\right]
                   & = Var\left[\frac{1}{nh_n}\sum\limits_{i=1}^n
                  \left[\phi\left(\frac{x-x_i}{h_n}\right)\right] \right] \\
                   & = \frac{1}{n^2h_n^2}\sum\limits_{i=1}^n
                  Var\left[\phi\left(\frac{x-x_i}{h_n}\right)\right]      \\
                   & = \frac{1}{nh_n^2}
                  Var\left[\phi\left(\frac{x-v}{h_n}\right)\right]        \\
                   & = \frac{1}{nh_n^2}
                  \left\{\mathcal{E}\left[\phi^2\left(\frac{x-v}{h_n}\right)\right]-
                  \left\{\mathcal{E}\left[\phi\left(\frac{x-v}{h_n}\right)\right]\right\}^2\right\}
              \end{aligned}
          $$
          å…¶ä¸­
          $$
              \begin{aligned}
                  \mathcal{E}\left[\phi^2\left(\frac{x-v}{h_n}\right)\right]
                   & = \int \phi^2\left(\frac{x-v}{h_n}\right)p\left(v\right)dv      \\
                   & = \int_{-infty}^{+infty}\frac{1}{2\pi}
                  exp\left[-\frac{1}{2}\left(\frac{x-v}{h_n}\right)^22\right]\frac{1}{\sqrt{2\pi}\sigma}
                  exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]dv  \\
                   & \overset{h_n/\sqrt{2}->h_n}{=} \frac{h_n/\sqrt{2}}{\sqrt{2\pi}}
                  \frac{1}{h_n/\sqrt{2}}\int_{-\infty}^{+\infty} \frac{1}{
                      \sqrt{2\pi}}exp\left[-\frac{1}{2}\left(\frac{x-v}{h_n/\sqrt{2}}\right)^2\right]\frac{1}
                  {\sqrt{2\pi}\sigma}exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]dv
              \end{aligned}
          $$
          æ ¹æ®å¼å­(1)å’Œ(2)ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š
          $$
              \begin{aligned}
                  \mathcal{E}\left[\phi^2\left(\frac{x-v}{h_n}\right)\right]
                   & = \frac{h_n/\sqrt{2}}{\sqrt{2\pi}} \frac{1}{\sqrt{2\pi}
                      \sqrt{h_n^2/2+\sigma^2}}exp\left[-\frac{1}{2}\cdot
                  \frac{\left(x-\mu\right)^2}{h_n^2/2+\sigma^2}\right]       \\
                   & = \frac{h_n}{2\sqrt{\pi}}\frac{1}{\sqrt{2\pi}
                      \sqrt{h_n^2/2+\sigma^2}}exp\left[-\frac{1}{2}\cdot
                      \frac{\left(x-\mu\right)^2}{h_n^2/2+\sigma^2}\right]
              \end{aligned}
          $$
          æ‰€ä»¥å¯ä»¥å¾—åˆ°$Var\left[p_n\left(x\right)\right]$çš„ç¬¬1é¡¹ï¼š
          $$
              \begin{aligned}
                  \frac{1}{nh_n^2}\mathcal{E}\left[\phi^2\left(\frac{x-v}{h_n}\right)\right]
                   & = \frac{1}{nh_n^2}\frac{h_n}{2\sqrt{\pi}}\frac{1}{\sqrt{2\pi}
                      \sqrt{h_n^2/2+\sigma^2}}exp\left[-\frac{1}{2}\cdot
                  \frac{\left(x-\mu\right)^2}{h_n^2/2+\sigma^2}\right]                           \\
                   & = \frac{1}{2nh_n\sqrt{\pi}}\frac{1}{\sqrt{2\pi}
                      \sqrt{h_n^2/2+\sigma^2}}exp\left[-\frac{1}{2}\cdot
                  \frac{\left(x-\mu\right)^2}{h_n^2/2+\sigma^2}\right]                           \\
                   & \approx \frac{1}{2nh_n\sqrt{\pi}}\left\{\frac{1}{\sqrt{2\pi}\sigma}
                  exp\left[-\frac{1}{2}\cdot\frac{\left(x-\mu\right)^2}{\sigma^2}\right]\right\} \\
                   & = \frac{1}{2nh_n\sqrt{\pi}}p\left(x\right)
              \end{aligned}
          $$
          ä¸Šå¼å­$\approx$æ˜¯å› ä¸º$\sqrt{h_n^2/2+\sigma^2}\approx\sigma$ã€‚\\
          ç±»ä¼¼åœ°$Var\left[p_n\left(x\right)\right]$çš„ç¬¬2é¡¹ï¼š
          $$
              \begin{aligned}
                  \frac{1}{nh_n^2}\left\{\mathcal{E}\left[\phi\left(\frac{x-v}{h_n}\right)\right]\right\}^2
                   & = \frac{1}{nh_n^2}\left[\int_{-\infty}^{+\infty}\phi\left(\frac{x-v}{h_n}\right)
                  p\left(v\right)dv\right]^2                                                          \\
                   & = \frac{1}{nh_n^2}\left\{ \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi}}
                  exp\left[-\frac{1}{2}\left(\frac{x-v}{h_n}\right)^2\right]\frac{1}{\sqrt{2\pi}\sigma}
                  exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]dv\right\}^2         \\
                   & = \frac{1}{nh_n^2}\left\{ h_n\frac{1}{\sqrt{2\pi}\sqrt{h_n^2+\sigma^2}}
                  exp\left[-\frac{1}{2}\cdot\frac{\left(x-\mu\right)^2}{h_n^2+\sigma^2}\right]
                  \right\}^2                                                                          \\
                   & = \frac{h_n^2}{nh_n^2}\left\{\frac{1}{\sqrt{2\pi}\sqrt{h_n^2+\sigma^2}}
                  exp\left[-\frac{1}{2}\cdot\frac{\left(x-\mu\right)^2}{h_n^2+\sigma^2}\right]
                  \right\}^2                                                                          \\
                   & \approx \frac{1}{n} \left\{\frac{1}{\sqrt{2\pi}\sigma}
                  exp\left[-\frac{1}{2}\cdot\frac{\left(x-\mu\right)^2}{\sigma^2}\right]\right\}^2    \\
                   & = \frac{1}{n}p^2\left(x\right)                                                   \\
                   & \approx 0
              \end{aligned}
          $$
          $\therefore Var\left[p_n\left(x\right)\right] \approx \frac{1}{2nh_n\sqrt\pi}p\left(x\right)$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Queestion 6}
Explore the effect of r on the accuracy of nearest-neighbor search based on par-
tial distance. Assume we have a large number n of points randomly placed in a d-
dimensional hypercube. Suppose we have a test point x, also selected randomly
in the hypercume, and find its nearest neighbor. By definition, if we use the full
d-dimensional Euclidean distance, we are guaranteed to find its nearest neigh-
bor. Suppose though we use the partial distance
$$
    D_r\left(x,x'\right) = \left(\sum\limits_{i=1}^r\left(x_i-x'_i\right)^2\right)^{1/2}
$$
\begin{enumerate}
    \item Plot the probability that a partial distance search finds the true closest neighbor of
          an arbitrary point x as a function of r for fixed $ n \left(1 \leq r \leq d\right)$
          for $d = 10$.
    \item Consider the effect of r on the accuracy of a nearest-neighbor classifier. Assume we
          have n/2 prototypes from each two categories in a hypercube of length 1 on a side.
          The density for each category is separable into the product of (linear) ramp functions,
          highest at one side, and zero at the other side of the range. Thus the density for category
          $\omega_1$  is highest at $\left(0,0,...0\right)^t$  and zero at $\left(1,1,...1\right)^t$,
          while the density for $\omega_2$ is highest at $\left(1,1,...1\right)^t$ and zero at
          $\left(0,0,...0\right)^t$ . State by inspection the Bayesian decision boundary.
\end{enumerate}

\section*{Answer 6}
è§£ï¼š
\begin{enumerate}
    \item
          å‡è®¾$n$ä¸ªæ ·æœ¬åœ¨$d$ç»´ç›¸äº’ç‹¬ç«‹ï¼Œå‡åŒ€åˆ†å¸ƒï¼Œ\\
          åˆ™æ¯ä¸ªç»´åº¦çš„æ ·æœ¬æ•°ä¸ºï¼š$n^{1/d}$ \\
          $r$ç»´ä¸Šçš„æ ·æœ¬æ•°ä¸ºï¼š$n^{r/d}$ \\
          $\therefore r$ç»´ä¸Šé€‰ä¸­æ­£ç¡®æ•°æ®ä½œä¸ºæœ€è¿‘é‚»çš„æ¦‚ç‡ä¸ºï¼š\\
          $$\begin{aligned}p =  n^{r/d}/n = n^{\frac{r}{d}-1}\end{aligned}$$
          $\because d = 10$\\
          $\therefore p = n^{\frac{r}{10}-1}$ \\
          å½“ $n = 2,3,4,...,10$ï¼Œ$p-r$å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š\\
          \begin{figure}[H]
              \centering
              \includegraphics[width=.8\textwidth]{hw2_2.png}
              \caption{ $p-r$å›¾ }
              \label{img2}
          \end{figure}

    \item
          1ç»´ï¼š$x_1^*=0.5$ \\
          2ç»´ï¼š$x_2^*=1-x_1$ \\
          3ç»´ï¼š$x_1x_2x_3 = \left(1-x_1\right)\left(1-x_2\right)\left(1-x_3\right)
              \Rightarrow x_3^* = \frac{\left(1-x_1\right)\left(1-x_2\right)}{1-x_1-x_2+2x_1x_2}$ \\
          ... \\
          dç»´ï¼š$\prod\limits_{i=1}^dx_i = \prod\limits_{i=1}^d\left(1-x_i\right)$
\end{enumerate}

\end{document}